# ETL-project-using-apache-pyspark-and-airflow
Create an ETL pipeline using PySpark to process daily sales data stored in AWS S3. Use Airflow to schedule and automate the ETL jobs, transforming the data and loading it into Amazon Redshift. Generate daily sales reports by querying Redshift to track performance.
